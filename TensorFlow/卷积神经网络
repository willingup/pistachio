#CNN 输入的图片-卷积-池化-全连接的层-分类器
#goole自己出了教程

代码不完整，但是cnn重点部分应该是这个

#定义用到的函数
def weight_variable(shape):
    inital = tf.truncated_normal(shape,stddev=0.1)  
    #tf.truncated_normal 中shape表示生成张量的维度，stddev表示标准差
    return tf.Variable

def bias_variable(shape):
    inital = tf.constant(0.1,shape=shape)
    return tf.Variable(inital)

def conv2d(x,W): #x为图片输入的值，W为卷积核
    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME') #strides 是步长 SAME是考虑边界
def max_pool_2x2(x):
    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME') #池化函数

   
#define placeholder for input to network
xs = tf.placeholder(tf.float32,[None,784]) # 28x28 不规定它有多少个但是规定每一个数据的大小
ys = tf.placeholder(tf.float32,[None,10])
keep_prob = tf.placeholder(tf.float32)
x_image = tf.reshape(xs,[-1,28,28,1]) #reshape 的-1其实就是整个的reshape的samples
#print(x_image.shape) #


#conv1 layer
W_conv1 = weight_variable([5,5,1,32]) #patch 5x5 in size 1;out size 32
b_conv1 = bias_variable([32])
h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1) # x_image是输入的图像
h_pool1 = max_pool_2x2(h_conv1)

#conv2 layer
W_conv2 = weight_variable([5,5,32,64]) 
b_conv2 = bias_variable([64])
h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)  #output size 14x14x64
h_pool2 = max_pool_2x2(h_conv2)                        #output size 7x7x64

#func1 layer
W_fc1 = weight_variable([7*7*64,1024])
b_fc1 = bias_variable([1024])
h_pool2_flat = tf.reshape(h_pool2,[-1,])                  #reshape函数的作用是调整矩阵的维度
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)   #tf.nn.relu 是激活函数，tf.matmul 矩阵相乘
h_fcl_drop = tf.nn.dropout(h_fcl,keep_prob)

#func2 layer
W_fc2 = weight_variable(1024,10)
b_fc2 = bias_variable([10])
prediction = tf.nn.softmax(tf.matmul(h_fcl_drop,W_fc2)+b_fc2)





#### 补充   
卷积函数 tf.nn.conv2d (input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)
参数： 
    input : 输入的要做卷积的图片，要求为一个张量，shape为 [ batch, in_height, in_weight, in_channel ]，其中batch为图片的数量，in_height 为图片高度，in_weight 为图片宽度，in_channel 为图片的通道数，灰度图该值为1，彩色图为3。（也可以用其它值，但是具体含义不是很理解）
    filter：卷积核，要求也是一个张量，shape为 [ filter_height, filter_weight, in_channel, out_channels ]，其中 filter_height 为卷积核高度，filter_weight 为卷积核宽度，in_channel 是图像通道数 ，和 input 的 in_channel 要保持一致，out_channel 是卷积核数量。
    strides：卷积时在图像每一维的步长，这是一个一维的向量，[ 1, strides, strides, 1]，第一位和最后一位固定必须是1
    padding：string类型，值为“SAME” 和 “VALID”，表示的是卷积的形式，是否考虑边界。"SAME"是考虑边界，不足的时候用0去填充周围，"VALID"则不考虑
    use_cudnn_on_gpu： bool类型，是否使用cudnn加速，默认为true
    
池化函数 tf.nn.max_pool(value, ksize, strides, padding, name=None)
参数：
    value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map，依然是[batch, height, width, channels]这样的shape
    ksize：池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1
    strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]
    padding：和卷积类似，可以取'VALID' 或者'SAME'
    返回一个Tensor，类型不变，shape仍然是[batch, height, width, channels]这种形式
    
函数：tf.nn.softmax(logits,axis=None,name=None,dim=None)
参数：logits：一个非空的Tensor，必须是下列类型之一：half， float32，float64
     axis：将在其上执行维度softmax。默认值为-1，表示最后一个维度
     name：操作的名称(可选)
     dim：axis的已弃用的别名
     返回：一个Tensor，与logits具有相同的类型和shape


